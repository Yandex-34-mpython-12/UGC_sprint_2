x-mongo-common: &mongo-common
  image: mongo:5.0.21
  healthcheck:
    test: echo 'db.runCommand("ping").ok' | mongo $(hostname)/test --quiet
    interval: 10s
    timeout: 5s
    retries: 3
    start_period: 10s

services:
  admin:
    build: admin_service/app
    container_name: admin
    env_file: .env
    volumes:
      - ./admin_service/data/static:/opt/data/static
      - ./admin_service/data/media:/opt/data/media
    depends_on:
      - films_db
      - elasticsearch
    expose:
      - 8000

  films:
    build: films_service
    container_name: films
    env_file: .env
    volumes:
      - ./films_service:/app
      - films_logs:/app/logs
    expose:
      - 8000
    depends_on:
      - etl
      - elasticsearch
      - redis
      - tracer
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/api/health/ || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  films_db:
    image: bitnami/postgresql:16
    container_name: films_db
    env_file: .env
    environment:
      - POSTGRESQL_DATABASE=${POSTGRES_DB_FILMS}
      - POSTGRESQL_USERNAME=${POSTGRES_USER_FILMS}
      - POSTGRESQL_PASSWORD=${POSTGRES_PASSWORD_FILMS}
      - POSTGRESQL_MASTER_HOST=${DB_HOST_FILMS}
      - POSTGRESQL_MASTER_PORT_NUMBER=${DB_PORT_FILMS}
    volumes:
      - films_db_data:/var/lib/postgresql/data
      - ./db/db_dump_260624.sql:/docker-entrypoint-initdb.d/db_dump_260624.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d films_db -U postgres"]
      interval: 2s
      timeout: 10s
      retries: 30

  auth:
    build: auth_service
    container_name: auth
    env_file: .env
    volumes:
      - ./auth_service:/app
      - auth_logs:/app/logs
    expose:
      - 8000
    depends_on:
      - auth_db
      - redis
      - tracer
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -f http://auth:${AUTH_CONFIG__RUN__PORT}/api/v1/health/ || exit 1",
        ]
      interval: 10s
      timeout: 5s
      retries: 5

  auth_db:
    image: bitnami/postgresql:16
    container_name: auth_db
    env_file: .env
    volumes:
      - auth_db_data:/var/lib/postgresql/data
    environment:
      - POSTGRESQL_DATABASE=${AUTH_CONFIG__DB__DB_NAME}
      - POSTGRESQL_USERNAME=${AUTH_CONFIG__DB__USER}
      - POSTGRESQL_PASSWORD=${AUTH_CONFIG__DB__PASSWORD}
      - POSTGRESQL_MASTER_HOST=${AUTH_CONFIG__DB__HOST}
      - POSTGRESQL_MASTER_PORT_NUMBER=${AUTH_CONFIG__DB__PORT}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d auth_db -U postgres"]
      interval: 2s
      timeout: 10s
      retries: 30

  bigdata:
    build: bigdata_service
    container_name: bigdata
    env_file: .env
    volumes:
      - ./bigdata_service:/app
      - bigdata_logs:/app/logs
    expose:
      - 8000
    ports:
      - 8000:8000
    depends_on:
      mongos:
        condition: service_healthy
      kafka-0:
        condition: service_healthy
    restart: always
    healthcheck:
      test:
        ["CMD-SHELL", "curl -f http://localhost:8000/api/v1/health/ || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7.4.0
    container_name: redis
    env_file: .env
    volumes:
      - redis_data:/data
    expose:
      - 6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    depends_on:
      - elasticsearch

  elasticsearch:
    build: "./es"
    container_name: elasticsearch
    expose:
      - 9200
    volumes:
      - ./es/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro
      - es_data:/usr/share/elasticsearch/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${ELASTICSEARCH_PORT}"]
      interval: 10s
      timeout: 5s
      retries: 5

  etl:
    build: ./etl
    container_name: etl
    env_file: .env
    volumes:
      - ./etl/es_state.json:/usr/src/app/es_state.json
    depends_on:
      - films_db
      - elasticsearch
    restart: always

  ugc_etl:
    build: ./ugc_etl
    container_name: ugc_etl
    env_file: .env
    depends_on:
      clickhouse:
        condition: service_healthy
      kafka-0:
        condition: service_healthy
    restart: always

  tracer:
    image: jaegertracing/all-in-one:latest
    container_name: tracer
    env_file: .env
    expose:
      - "9411"
      - "6831/udp"
    command:
      ["--collector.zipkin.host-port=0.0.0.0:${COLLECTOR_ZIPKIN_HTTP_PORT}"]
    restart: on-failure

  nginx:
    image: nginx:1.27.1
    container_name: nginx
    env_file: .env
    ports:
      - "80:80"
    volumes:
      - ./admin_service/data:/data/:ro
      - ./admin_service/data/static:/static
      - ./admin_service/data/media:/media
      - ./etc/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./etc/nginx/conf.d:/etc/nginx/conf.d:ro
    depends_on:
      - admin
      - films
      - auth

  kafka-0:
    build: ./kafka
    container_name: kafka-0
    env_file:
      - .env
      - ./kafka/env/kafka_0.env
    volumes:
      - kafka-0_data:/bitnami/kafka
    depends_on:
      kafka-1:
        condition: service_healthy
      kafka-2:
        condition: service_healthy
    healthcheck:
      test: kafka-cluster.sh cluster-id --bootstrap-server localhost:9092 || exit 1
      interval: 1s
      timeout: 60s
      retries: 60

  kafka-1:
    build: ./kafka
    container_name: kafka-1
    env_file:
      - .env
      - ./kafka/env/kafka_1.env
    volumes:
      - kafka-1_data:/bitnami/kafka
    healthcheck:
      test: kafka-cluster.sh cluster-id --bootstrap-server localhost:9092 || exit 1
      interval: 1s
      timeout: 60s
      retries: 60

  kafka-2:
    build: ./kafka
    container_name: kafka-2
    env_file:
      - .env
      - ./kafka/env/kafka_2.env
    volumes:
      - kafka-2_data:/bitnami/kafka
    healthcheck:
      test: kafka-cluster.sh cluster-id --bootstrap-server localhost:9092 || exit 1
      interval: 1s
      timeout: 60s
      retries: 60

  kafka-ui:
    image: provectuslabs/kafka-ui:v0.7.2
    container_name: kafka-ui
    env_file:
      - .env

  clickhouse:
    image: yandex/clickhouse-server:latest
    container_name: clickhouse
    volumes:
      - clickhouse_data:/var/lib/clickhouse
    healthcheck:
      test: wget --no-verbose --tries=1 --spider http://localhost:8123/ping || exit 1

  filebeat-auth-api:
    build:
      context: ./auth_service/infra/filebeat
    container_name: filebeat_auth
    volumes:
      - auth_logs:/var/app/log
    depends_on:
      - auth
      - kibana
      - elasticsearch-logs

  filebeat-films-api:
    build:
      context: ./films_service/infra/filebeat
    container_name: filebeat_films
    volumes:
      - films_logs:/var/app/log
    depends_on:
      - films
      - kibana
      - elasticsearch-logs

  filebeat-bigdata-api:
    build:
      context: ./bigdata_service/infra/filebeat
    container_name: filebeat_bigdata
    volumes:
      - bigdata_logs:/var/app/log
    depends_on:
      - bigdata
      - kibana
      - elasticsearch-logs

  logstash:
    image: logstash:8.10.2
    depends_on:
      - elasticsearch-logs
    environment:
      XPACK_MONITORING_ENABLED: "false"
      ES_HOST: "elasticsearch-logs:9200"
    expose:
      - 5044/udp
    volumes:
      - ./deploy/logstash.conf:/config/logstash.conf:ro
    command: logstash -f /config/logstash.conf

  # Обратите внимание: не стоит использовать для ELK тот же ES, который задействован для полнотекстового поиска в вашем сервисе
  elasticsearch-logs:
    image: elasticsearch:8.10.2
    environment:
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms265m -Xmx265m"
      - xpack.security.enabled=false
    volumes:
      - es-logs-data:/tmp/elasticsearch/data

  kibana:
    image: kibana:8.10.2
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch-logs
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch-logs:9200

  mongo_cnf_n1:
    <<: *mongo-common
    hostname: mongo_cnf_n1
    container_name: mongo_cnf_n1
    command: mongod --configsvr --replSet mongo_cnf --dbpath /data/db --port 27017 --bind_ip localhost,mongo_cnf_n1
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - mongo_cnf_n1_data:/data/db

  mongo_cnf_n2:
    <<: *mongo-common
    hostname: mongo_cnf_n2
    container_name: mongo_cnf_n2
    command: mongod --configsvr --replSet mongo_cnf --dbpath /data/db --port 27017 --bind_ip localhost,mongo_cnf_n2
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - mongo_cnf_n2_data:/data/db

  config_cnf_rs:
    <<: *mongo-common
    volumes:
      - ./mongo_config/rs_cnf_setup.js:/conf/setup.js
    command:
      ["mongosh", "--host", "mongo_cnf_n1", "--port", "27017", "/conf/setup.js"]
    depends_on:
      mongo_cnf_n1:
        condition: service_healthy
      mongo_cnf_n2:
        condition: service_healthy

  mongo_rs1_n1:
    <<: *mongo-common
    hostname: mongo_rs1_n1
    container_name: mongo_rs1_n1
    command: mongod --shardsvr --replSet mongo_rs1 --dbpath /data/db --port 27017 --bind_ip localhost,mongo_rs1_n1
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - mongo_rs1_n1_data:/data/db

  mongo_rs1_n2:
    <<: *mongo-common
    hostname: mongo_rs1_n2
    container_name: mongo_rs1_n2
    command: mongod --shardsvr --replSet mongo_rs1 --dbpath /data/db --port 27017 --bind_ip localhost,mongo_rs1_n2
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - mongo_rs1_n2_data:/data/db

  config_rs1:
    <<: *mongo-common
    volumes:
      - ./mongo_config/rs_1_setup.js:/conf/setup.js
    command:
      ["mongosh", "--host", "mongo_rs1_n1", "--port", "27017", "/conf/setup.js"]
    depends_on:
      mongo_rs1_n1:
        condition: service_healthy
      mongo_rs1_n2:
        condition: service_healthy

  mongo_rs2_n1:
    <<: *mongo-common
    hostname: mongo_rs2_n1
    container_name: mongo_rs2_n1
    command: mongod --shardsvr --replSet mongo_rs2 --dbpath /data/db --port 27017 --bind_ip localhost,mongo_rs2_n1
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - mongo_rs2_n1_data:/data/db

  mongo_rs2_n2:
    <<: *mongo-common
    hostname: mongo_rs2_n2
    container_name: mongo_rs2_n2
    command: mongod --shardsvr --replSet mongo_rs2 --dbpath /data/db --port 27017 --bind_ip localhost,mongo_rs2_n2
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - mongo_rs2_n2_data:/data/db

  config_rs2:
    <<: *mongo-common
    volumes:
      - ./mongo_config/rs_2_setup.js:/conf/setup.js
    command:
      ["mongosh", "--host", "mongo_rs2_n1", "--port", "27017", "/conf/setup.js"]
    depends_on:
      mongo_rs2_n1:
        condition: service_healthy
      mongo_rs2_n2:
        condition: service_healthy

  mongos:
    <<: *mongo-common
    command: mongos --configdb mongo_cnf/mongo_cnf_n1:27017,mongo_cnf_n2:27017 --port 27017 --bind_ip localhost,mongos
    hostname: mongos
    container_name: mongos
    ports:
      - "27017:27017"
    environment:
      - MONGO_INITDB_DATABASE=posts_db
    depends_on:
      config_rs1:
        condition: service_completed_successfully
      config_rs2:
        condition: service_completed_successfully
    volumes:
      - /etc/localtime:/etc/localtime:ro

  config_shared_cluster:
    <<: *mongo-common
    command:
      ["mongosh", "--host", "mongos", "--port", "27017", "/conf/setup.js"]
    volumes:
      - ./mongo_config/sharded_cluster_setup.js:/conf/setup.js
    depends_on:
      mongos:
        condition: service_healthy

volumes:
  films_db_data:
  auth_db_data:
  redis_data:
  es_data:
  kafka-0_data:
  kafka-1_data:
  kafka-2_data:
  clickhouse_data:
  es-logs-data:
  auth_logs:
  films_logs:
  bigdata_logs:
  mongo_rs1_n1_data:
  mongo_rs1_n2_data:
  mongo_rs2_n1_data:
  mongo_rs2_n2_data:
  mongo_cnf_n1_data:
  mongo_cnf_n2_data:
